# Кластеризация

Относится к обучению без учителя -- на данных нет разметки о принадлежности к тому или иному классу (кластеру). Алгоритм сам в процессе обучения производит разделение на классы

# KMeans
### Идея метода

0. Случайно выставляем точки в N-мерном пространстве в допустимой области, где `N` - число признаков в данных. Число признаков можно найти как `df.shape[1]`. Данные точки будут центрами кластеров (**центроидами**) `0..N-1`. Присваиваем число итераций, равное нулю `n_iter = 0`.
1. Кластеризуем точки. На данном шаге вычисляются все попарные расстояния между центрами кластеров `center` и точками `P` в датасете. Функция расстояния - Евклидова (корень из суммы квадратов). Для каждой точки в датасете присваивается номер кластера, к которому она принадлежит - класс ближайшей из точек `center`.
2. Если на данном этапе количество итераций больше либо равно максимальному числу итераций `n_iter >= max_iter`, возвращаем результат кластеризации (то есть наши предсказания классов), иначе повышаем `n_iter += 1`.
3. Пересчитываем центры кластеров. Для каждой координаты высчитываем взвешенную сумму координат тех точек, которые попали в данный кластер. **НЕ нужно пользоваться евклидовой метрикой на данном шаге.** Мы берем среднее для каждой из координат среди всех тех точек, которые попали в кластер (усредняем координаты всех точек кластера). Если для всех центров кластеров их координаты изменились незначительно, прерываем вычисления (алгоритм сошелся), возвращаем результат. Иначе переходим к шагу 1.

```python
from sklearn.cluster import KMeans
```
```python
kmeans = KMeans(n_clusters=3, n_init='auto').fit(data) # обучаем
```
```python
kmeans.cluster_centers_ # координаты центроидов
```
**Подойдёт для двумерного случая -- отрисовка кластеров на графике:**
```python
# отрисовываем на плоскости точки, цветом обозначаем кластер
plt.scatter(data[:,0], data[:,1], c=kmeans.labels_)
# отрисовываем центроиды
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=100, c='red')
```
```python
kmeans.labels_ # смотрим, к каким кластерам отнеслись точки
```
# Метрики
Меры оценки кластеризации бывают двух видов:

- Внутренние -- меры отображают качество кластеризации только по информации в данных

- Внешние -- меры основаны на сравнении результата кластеризации с априори известным разделением на классы  

Самые частые:  
- **Внутренние:**  
    - silhouette  
Коэффициент силуэта рассчитывается с использованием среднего расстояния внутри кластера ( a) и среднего расстояния до ближайшего кластера ( b) для каждой выборки. (b - a) / max(a, b). Лучшее значение 1, а худшее -1.
        ```python
        from sklearn.metrics import silhouette_score
        silhouette_score(X, labels_pred)
        ```
    - inertia
 Сумма квадратов расстояний для всех точек до ближайших к ним центроидов
        ```python
        kmeans.inertia_
        ```  
- **Внешние:** (для данных, у которых размечено истинное значение целевого признака, как в классификации)
    - rand score  
Вычисляет меру подобия между двумя кластеризациями, рассматривая все пары выборок и подсчитывая пары, которые назначены в одном и том же или разных кластерах в предсказанных и истинных кластеризациях
        ```python
        from sklearn.metrics.cluster import rand_score
        rand_score(labels_true, labels_pred)
        ```
## Kmeans: подбор числа кластеров
### inertia
```python
kmeans.inertia_
```
Известно и очевидно, что с увеличением количества кластеров эта сумма будет уменьшаться, однако в определенный момент (оптимальное количество кластеров), она начнет уменьшаться слишком медлено.

Считаем для каждого числа кластеров от 2 до 10 значение inertia
```python
inertias = []

for i in range(2, 11):
  km = KMeans(i, n_init='auto').fit(data)
  inertias.append(km.inertia_)
```
```python
plt.plot(range(2, 11), inertias) # рисуем их на графике
```
На графике необходимо найти "локоть" -- точку, после которой значения начинают снижаться незначительно. Это и будет оптимальное число кластеров (не всегда можно точно найти одну конкретную точку, можно указать примерный диапазон из 2-3 значений, в общем зависит от поведения полученного графика -- ищем где график преломляется и указываем где это происходит)

### silhouette
```python
silhouette_score(data, kmeans.labels_)
```
Считаем для каждого числа кластеров от 2 до 10 значение silhouette
```python
ss = []

for i in range(2, 11):
  ss.append(silhouette_score(data, KMeans(i, n_init='auto').fit(data).labels_))
```
рисуем
```python
plt.plot(range(2, 11), ss)
```
Формально чем выше значение этой метрики - тем лучше. То есть ориентируемся на пиковое значение графика

# Гауссовы смеси (gaussian mixture)
Другой моделью для кластеризации являются гауссовые смеси, идея которых в том, что кластер для точки будет определяться не по расстоянию до центроида, а по вероятности (для каждой точки мы будем знать вероятности ее принадлежности к каждому из кластеров - выбираем кластер, соответствующий максимальной вероятности)
```python
from sklearn.mixture import GaussianMixture

gm = GaussianMixture(n_components=3).fit(data) # обучаем
```
```python
gm.means_ # координаты центроидов
```
```python
gm.predict(data) # смотрим, к каким кластерам отнеслись точки
```
Аналогично с KMeans, можем визуализировать, если размерность 2:
```python
plt.scatter(data[:,0], data[:,1], c=gm.predict(data[0]))
plt.scatter(gm.means_[:,0], gm.means_[:,1], s=100, c='red')
```

# dbscan

DBSCAN (Density-based spatial clustering of applications with noise) - это наиболее часто используемый в настоящее время алгоритм кластеризации (вернее сказать, что это база, поскаольку наиболее часто на практике испольхуются его модификации).
Данный алгоритм предполагает, что часть точек из набора данных не принадлежат ни одному из кластеров, а являются шумом. В то же время никаких центроидов нет - кластеры формируются на основе близости.
```python
from sklearn.cluster import DBSCAN
```
```python
dbscan = DBSCAN().fit(data) # обучаем
```
```python
dbscan.labels_ # смотрим, к каким кластерам отнеслись точки; -1 - точки, помеченные как шум
```
Двумерный случай можем визуализировать по аналогии с моделями описанными выше, только здесь отсутствуют центроиды
```python
plt.scatter(data[:,0], data[:,1], c=dbscan.labels_)
```
### dbscan: Подбор гиперпараметров
Основными параметрами алгоритма являются эпсилон и минимальное количество точек в окрестности (с радиусом эпсилон).
Минимальное количество точек в окрестности часто берут как 2 * размерность данных

Далее для каждой точки считают расстояния до k ближайших соседей (k в данном случае равно 4), считая за первого соседа саму точку (иногда не считают саму точку за первого соседа - как захочет исследователь). Полученные значения усредняют

Усредненные значения сортируют по возрастанию и отображают на графике. Знакомая вам локтевая точка (она определяется исследователем по виду графика) может указать на значение эпсилон (это будет координата y)

```python
from sklearn.neighbors import NearestNeighbors

nbrs = NearestNeighbors(n_neighbors=4).fit(data)
distances, indices = nbrs.kneighbors(data)
distances
```
усредняем distances
```python
distances = np.mean(distances, axis=1)
distances = np.sort(distances)
distances
```
```python
plt.plot(distances) # визуализируем и ищем "локтевую" точку - ту, где график сильно перегибается
```
значение по вертикальной оси -- искомый эпсилон
**Обучаем лучшую версию модели с подобранными гиперпараметрами**
```python
dbscan = DBSCAN(eps=0.8, min_samples=4).fit(data)
print(np.unique(dbscan.labels_))
plt.scatter(data[:,0], data[:,1], c=dbscan.labels_)
```

Можно и силуэт посчитать
```python
silhouette_score(data, dbscan.labels_)
```